{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002f9cc0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464670da",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767ccf3f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8988ae1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/haroldinho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /Users/haroldinho/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7002025834a94cf3a37a3f96c0efd521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 04:07:16 INFO: Downloading default packages for language: pt (Portuguese)...\n",
      "2022-03-24 04:07:17 INFO: File exists: /Users/haroldinho/stanza_resources/pt/default.zip.\n",
      "2022-03-24 04:07:18 INFO: Finished downloading models and saved to /Users/haroldinho/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import warnings  # for life!\n",
    "warnings.filterwarnings('ignore')\n",
    "import unidecode  # 1.7\n",
    "import pandas as pd  # for life!\n",
    "import re  # 1.5\n",
    "import string  # Common string operations\n",
    "import nltk  # 1.14\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize  # 1.12 - 1.19\n",
    "from nltk.corpus import stopwords  # 1.12\n",
    "from nltk.stem import WordNetLemmatizer # 1.14\n",
    "from nltk.stem import RSLPStemmer # 1.14\n",
    "nltk.download('rslp')\n",
    "from autocorrect import Speller # 1.13\n",
    "from bs4 import BeautifulSoup  # 1.4\n",
    "import stanza  # 1.15\n",
    "stanza.download('pt')  # 1.15\n",
    "# import time\n",
    "#from nltk.stem.api import StemmerI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e4584",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f5f5a9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Data points:  159849\n",
      "Number of Features:  3\n",
      "features:  ['Unnamed: 0' 'tweet_text' 'encoded_sentiment']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@dilsonramoslima #Fato Acho que o Roger Ã© um b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#NOVIDADE! @LATAM_BRA acaba de anunciar novo v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Quando tem #novidadeðŸ˜† tem @novafm103 na Ã¡rea! ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@RiodeNojeira #Novidade TaÃ­ o sucesso dos filh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Livro/Novidades] Segredos, uma histÃ³ria de Lu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         tweet_text  \\\n",
       "0           0  @dilsonramoslima #Fato Acho que o Roger Ã© um b...   \n",
       "1           1  #NOVIDADE! @LATAM_BRA acaba de anunciar novo v...   \n",
       "2           2  Quando tem #novidadeðŸ˜† tem @novafm103 na Ã¡rea! ...   \n",
       "3           3  @RiodeNojeira #Novidade TaÃ­ o sucesso dos filh...   \n",
       "4           4  [Livro/Novidades] Segredos, uma histÃ³ria de Lu...   \n",
       "\n",
       "   encoded_sentiment  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Dataset\n",
    "df = pd.read_csv('../thermofeeler/data/encoded_df.csv')\n",
    "print('Number of Data points: ', df.shape[0])\n",
    "print('Number of Features: ', df.shape[1])\n",
    "print('features: ', df.columns.values)\n",
    "\n",
    "# Show Dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2c969a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@dilsonramoslima #Fato Acho que o Roger Ã© um b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#NOVIDADE! @LATAM_BRA acaba de anunciar novo v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quando tem #novidadeðŸ˜† tem @novafm103 na Ã¡rea! ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  encoded_sentiment\n",
       "0  @dilsonramoslima #Fato Acho que o Roger Ã© um b...                  0\n",
       "1  #NOVIDADE! @LATAM_BRA acaba de anunciar novo v...                  0\n",
       "2  Quando tem #novidadeðŸ˜† tem @novafm103 na Ã¡rea! ...                  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unnamed column\n",
    "df = df.drop(columns='Unnamed: 0')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be74bbb3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159849 entries, 0 to 159848\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   tweet_text         159849 non-null  object\n",
      " 1   encoded_sentiment  159849 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# This command tells information about the attributes of Dataset.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f694be2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159849.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.816499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       encoded_sentiment\n",
       "count      159849.000000\n",
       "mean            0.000000\n",
       "std             0.816499\n",
       "min            -1.000000\n",
       "25%            -1.000000\n",
       "50%             0.000000\n",
       "75%             1.000000\n",
       "max             1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows statistics for every numerical column in our dataset.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1a595",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Check type of Dataframe attribute that has to processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937d11e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type of attribute \"tweet_text\"\n",
    "type(df['tweet_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2e8858c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type of attribute \"encoded_sentiment\"\n",
    "type(df['encoded_sentiment'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71834486",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q lindo &lt;3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma instituiÃ§Ã£o que cuida de crianÃ§as que sofrem bullying, abuso e violÃªncia. pegaram a historia dele e fizeram um projeto pra ajudar crianÃ§as :) https://t.co/M2436o8y6E'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = df['tweet_text'][85442]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a837797",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Remove newlines & tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d11ccfbd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_newlines_tabs(tweet):\n",
    "    \"\"\"  \n",
    "    This function will remove all the occurrences of newlines, tabs, and combinations like: \\\\n, \\\\.\n",
    "    \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of newlines, tabs, \\\\n, \\\\ characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\n",
    "    Output : This is her first day at this place. Please, Be nice to her. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n",
    "    Formatted_tweet = tweet.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "    return Formatted_tweet\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7374e50a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is her   first day at this place.  Please,  Be nice to her. \n"
     ]
    }
   ],
   "source": [
    "print(remove_newlines_tabs('This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45533125",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Strip Html Tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2093ede0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# DESACTIVATE - We are using def Remove special characters\n",
    "# remove_html=False\n",
    "def strip_html_tags(tweet):  # '< >' for scraping\n",
    "    \"\"\" \n",
    "    This function will remove all the occurrences of html tags from the text.\n",
    "    \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of html tags.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is a nice place to live. <IMG>\n",
    "    Output : This is a nice place to live. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Initiating BeautifulSoup object soup.\n",
    "    soup = BeautifulSoup(tweet, \"html.parser\")\n",
    "    # Get all the text other than html tags.\n",
    "    stripped_tweet = soup.get_text(separator=\" \")\n",
    "    return stripped_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48345ca8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a nice place to live. '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_html_tags('This is a nice place to live. <IMG>') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a89b8",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Remove Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "814926c9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    \"\"\"\n",
    "    This function will remove all the occurrences of links.\n",
    "    \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of all types of links.\n",
    "        \n",
    "    Example:\n",
    "    Input : To know more about cats and food & website: catster.com  visit: https://catster.com//how-to-feed-cats\n",
    "    Output : To know more about cats and food & website: visit:     \n",
    "    \n",
    "    \"\"\"\n",
    "    # Removing all the occurrences of links that starts with https\n",
    "    remove_https = re.sub(r'http\\S+', '', tweet)  # r'\\1' --> Limits all the repeatation to only one character.                       \n",
    "    \n",
    "    # Remove all the occurrences of text that ends with .com\n",
    "    remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", ' ', remove_https)\n",
    "    return remove_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "085fbdb4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To know more about cats and food & website:   visit: '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_links('To know more about cats and food & website: catster.com  visit: https://catster.com//how-to-feed-cats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59117494",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Remove WhiteSpaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54181d40",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_whitespaces(tweet):\n",
    "    \"\"\" \n",
    "    This function will remove \n",
    "    extra white spaces from the text\n",
    "        \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after extra whitespaces removed .\n",
    "        \n",
    "    Example:\n",
    "    Input : How   are   you   doing     ?\n",
    "    Output : How are you doing ?     \n",
    "        \n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'\\s+')  # r'\\1' --> Limits all the repeatation to only one character.\n",
    "    Without_whitespace = re.sub(pattern, ' ', tweet)\n",
    "    # There are some instances where there is no space after '?' & ')', \n",
    "    # So I am replacing these with one space so that It will not consider two words as one token.\n",
    "    tweet = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85096788",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How are you doing  ? '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_whitespaces('How   are   you   doing     ?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a17014",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "326ac903",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_number(tweet):\n",
    "    \"\"\" \n",
    "    This function will remove \n",
    "    all numbers from the text\n",
    "        \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\" with fell numbers. \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of all numbers.\n",
    "        \n",
    "    Example:\n",
    "    Input : Hello my id. is 76483927 and my phone number is 67384902\n",
    "    Output : Hello my id. is   and my phone number is  \n",
    "    \n",
    "    \"\"\"\n",
    "    tweet = re.sub(r'[0-9]+',' ',tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db17b4eb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello my id. is   and my phone number is  '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_number('Hello my id. is 76483927 and my phone number is 67384902')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ca49d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Remove Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3696f74",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Code for accented characters removal\n",
    "def accented_characters_removal(tweet):\n",
    "    \"\"\"\n",
    "    The function will remove accented characters from the \n",
    "    text contained within the Dataset.\n",
    "       \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" with removed accented characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : MÃ¡laga, Ã Ã©ÃªÃ¶hello\n",
    "    Output : Malaga, aeeohello    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove accented characters from text using unidecode.\n",
    "    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "    tweet = unidecode.unidecode(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b59ce91c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Malaga, aeeohello'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accented_characters_removal('MÃ¡laga, Ã Ã©ÃªÃ¶hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637c9cd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6452e72",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Code for text lowercasing\n",
    "def lower_casing_text(tweet):\n",
    "    \"\"\"\n",
    "    The function will convert text into lower case.\n",
    "    \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "         value: text in lowercase\n",
    "         \n",
    "    Example:\n",
    "    Input : The World Is Full Of Surprises!\n",
    "    Output : the world is full of surprises!\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert text to lower case\n",
    "    # lower() - It converts all upperase letter of given string to lowercase.\n",
    "    tweet = tweet.lower()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b4ed883",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the world is full of surprises!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_casing_text('The World Is Full Of Surprises!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2406c84",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Reduce repeated characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a4a48fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Code for removing repeated characters and punctuations\n",
    "def reducing_error_char_repeatation(tweet):\n",
    "    \"\"\"\n",
    "    This Function will reduce repeatition to two characters \n",
    "    for alphabets.\n",
    "    \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Finally formatted text with alphabets repeating to \n",
    "        two characters & punctuations limited to one repeatition \n",
    "        \n",
    "    Example:\n",
    "    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
    "    Output : Reallyy, Greeaatt !?.;:)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Pattern matching for all case alphabets# Pattern matching for all case alphabets\n",
    "    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)  \n",
    "    # Limiting all the  repeatation to two characters.\n",
    "    Formatted_tweet = Pattern_alpha.sub(r\"\\1\\1\", tweet)   # r'\\1\\1' --> It limits all the repeatation to two characters.\n",
    "    \n",
    "    # This Function will reduce repeatition to one character for punctuations.\n",
    "    # Pattern matching for all the punctuations that can occur\n",
    "    # Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')  # [.,/#!$%^&*?;:{}=_`~()+-]\n",
    "    \n",
    "    # Limiting punctuations in previously formatted string to only one.\n",
    "    # Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_tweet)\n",
    "    \n",
    "    # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n",
    "    # Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)  # {2,} --> It means to match for repeatation that \n",
    "    #                                                                       occurs more than two times\n",
    "    return Formatted_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ac7b693",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reallyy,        Greeaatt   !!!!?....;;;;:)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducing_error_char_repeatation('Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d6185",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa232372",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(tweet):\n",
    "    \"\"\"\n",
    "    The function will remove all punctuaction from the \n",
    "    text contained within the Dataset.\n",
    "       \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" with removed accented characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : good morning! buen dia,\\.:|;!?`~+\n",
    "    Output : good morning buen dia   \n",
    "        \n",
    "    \"\"\"\n",
    "    tweet = re.sub(r'[^\\w\\s]',' ',tweet)  # remove punctuation\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d420b090",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good morning buen dia           '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation('good morning!buen dia,\\.:|;!?`~+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca6ed7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Expand contraction words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eae59800",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# English Word Mapping\n",
    "\n",
    "# CONTRACTION_MAP_EN = {\n",
    "# \"ain't\": \"is not\",\n",
    "# \"aren't\": \"are not\",\n",
    "# \"can't\": \"cannot\",\n",
    "# \"can't've\": \"cannot have\",\n",
    "# \"'cause\": \"because\",\n",
    "# \"could've\": \"could have\",\n",
    "# \"couldn't\": \"could not\",\n",
    "# \"couldn't've\": \"could not have\",\n",
    "# \"didn't\": \"did not\",\n",
    "# \"doesn't\": \"does not\",\n",
    "# \"don't\": \"do not\",\n",
    "# \"hadn't\": \"had not\",\n",
    "# \"hadn't've\": \"had not have\",\n",
    "# \"hasn't\": \"has not\",\n",
    "# \"haven't\": \"have not\",\n",
    "# \"he'd\": \"he would\",\n",
    "# \"he'd've\": \"he would have\",\n",
    "# \"he'll\": \"he will\",\n",
    "# \"he'll've\": \"he he will have\",\n",
    "# \"he's\": \"he is\",\n",
    "# \"how'd\": \"how did\",\n",
    "# \"how'd'y\": \"how do you\",\n",
    "# \"how'll\": \"how will\",\n",
    "# \"how's\": \"how is\",\n",
    "# \"i'd\": \"i would\",\n",
    "# \"i'd've\": \"i would have\",\n",
    "# \"i'll\": \"i will\",\n",
    "# \"i'll've\": \"i will have\",\n",
    "# \"i'm\": \"i am\",\n",
    "# \"i've\": \"i have\",\n",
    "# \"isn't\": \"is not\",\n",
    "# \"it'd\": \"it would\",\n",
    "# \"it'd've\": \"it would have\",\n",
    "# \"it'll\": \"it will\",\n",
    "# \"it'll've\": \"it will have\",\n",
    "# \"it's\": \"it is\",\n",
    "# \"let's\": \"let us\",\n",
    "# \"ma'am\": \"madam\",\n",
    "# \"mayn't\": \"may not\",\n",
    "# \"might've\": \"might have\",\n",
    "# \"mightn't\": \"might not\",\n",
    "# \"mightn't've\": \"might not have\",\n",
    "# \"must've\": \"must have\",\n",
    "# \"mustn't\": \"must not\",\n",
    "# \"mustn't've\": \"must not have\",\n",
    "# \"needn't\": \"need not\",\n",
    "# \"needn't've\": \"need not have\",\n",
    "# \"o'clock\": \"of the clock\",\n",
    "# \"oughtn't\": \"ought not\",\n",
    "# \"oughtn't've\": \"ought not have\",\n",
    "# \"shan't\": \"shall not\",\n",
    "# \"sha'n't\": \"shall not\",\n",
    "# \"shan't've\": \"shall not have\",\n",
    "# \"she'd\": \"she would\",\n",
    "# \"she'd've\": \"she would have\",\n",
    "# \"she'll\": \"she will\",\n",
    "# \"she'll've\": \"she will have\",\n",
    "# \"she's\": \"she is\",\n",
    "# \"should've\": \"should have\",\n",
    "# \"shouldn't\": \"should not\",\n",
    "# \"shouldn't've\": \"should not have\",\n",
    "# \"so've\": \"so have\",\n",
    "# \"so's\": \"so as\",\n",
    "# \"that'd\": \"that would\",\n",
    "# \"that'd've\": \"that would have\",\n",
    "# \"that's\": \"that is\",\n",
    "# \"there'd\": \"there would\",\n",
    "# \"there'd've\": \"there would have\",\n",
    "# \"there's\": \"there is\",\n",
    "# \"they'd\": \"they would\",\n",
    "# \"they'd've\": \"they would have\",\n",
    "# \"they'll\": \"they will\",\n",
    "# \"they'll've\": \"they will have\",\n",
    "# \"they're\": \"they are\",\n",
    "# \"they've\": \"they have\",\n",
    "# \"to've\": \"to have\",\n",
    "# \"wasn't\": \"was not\",\n",
    "# \"we'd\": \"we would\",\n",
    "# \"we'd've\": \"we would have\",\n",
    "# \"we'll\": \"we will\",\n",
    "# \"we'll've\": \"we will have\",\n",
    "# \"we're\": \"we are\",\n",
    "# \"we've\": \"we have\",\n",
    "# \"weren't\": \"were not\",\n",
    "# \"what'll\": \"what will\",\n",
    "# \"what'll've\": \"what will have\",\n",
    "# \"what're\": \"what are\",\n",
    "# \"what's\": \"what is\",\n",
    "# \"what've\": \"what have\",\n",
    "# \"when's\": \"when is\",\n",
    "# \"when've\": \"when have\",\n",
    "# \"where'd\": \"where did\",\n",
    "# \"where's\": \"where is\",\n",
    "# \"where've\": \"where have\",\n",
    "# \"who'll\": \"who will\",\n",
    "# \"who'll've\": \"who will have\",\n",
    "# \"who's\": \"who is\",\n",
    "# \"who've\": \"who have\",\n",
    "# \"why's\": \"why is\",\n",
    "# \"why've\": \"why have\",\n",
    "# \"will've\": \"will have\",\n",
    "# \"won't\": \"will not\",\n",
    "# \"won't've\": \"will not have\",\n",
    "# \"would've\": \"would have\",\n",
    "# \"wouldn't\": \"would not\",\n",
    "# \"wouldn't've\": \"would not have\",\n",
    "# \"y'all\": \"you all\",\n",
    "# \"y'all'd\": \"you all would\",\n",
    "# \"y'all'd've\": \"you all would have\",\n",
    "# \"y'all're\": \"you all are\",\n",
    "# \"y'all've\": \"you all have\",\n",
    "# \"you'd\": \"you would\",\n",
    "# \"you'd've\": \"you would have\",\n",
    "# \"you'll\": \"you will\",\n",
    "# \"you'll've\": \"you will have\",\n",
    "# \"you're\": \"you are\",\n",
    "# \"you've\": \"you have\",\n",
    "# }\n",
    "\n",
    "# Portuguese Word Mapping\n",
    "CONTRACTION_MAP_PT = {'Ã©':'ser','eh':'ser','vc':'voce','vcs':'voces','tb': 'tambem','tbm': 'tambem', \n",
    "            'obg': 'obrigado','obrigada':'obrigado','gnt': 'gente', 'q': 'que', 'n': 'nao', \n",
    "            'cmg': 'comigo', 'p':'para','pra' :'para','ta': 'estÃ¡','tÃ¡':'estÃ¡','to': 'estou', \n",
    "            'vdd':'verdade','bjos':'beijo','bjo':'beijo','kd': 'cade', 'pq':'porque',\n",
    "            'cmg':'comigo','cm':'com','pc':'ca','aq':'aqui','qdo':'quando','p':'para','':'que','agr':'agora'}\n",
    "\n",
    "\n",
    "# The code for expanding contraction words   \n",
    "def expand_contractions(tweet):  #, contraction_mapping =  CONTRACTION_MAP_PT):\n",
    "    \"\"\"\n",
    "    expand shortened words to the actual form.\n",
    "    e.g. don't  to  do not\n",
    "    \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "         value: Text with expanded form of shorthened words.\n",
    "        \n",
    "    Example: \n",
    "    Input : Vamos pra praia cmg qdo vc sair do trabalho, bjs e obg\n",
    "    Output : Vamos para praia comigo quando voce sair do trabalho, bjs e obrigado \n",
    "  \n",
    "    \"\"\"\n",
    "# Tokenizing text into tokens.\n",
    "    list_Of_tokens = tweet.split(' ')\n",
    "    for word in list_Of_tokens:\n",
    "        if word in CONTRACTION_MAP_PT.keys():\n",
    "            word_value = CONTRACTION_MAP_PT[word]\n",
    "            list_Of_tokens[list_Of_tokens.index(word)] = word_value\n",
    "            \n",
    "#   Converting list of tokens to String.\n",
    "    String_Of_tokens = ' '.join(i for i in list_Of_tokens) \n",
    "    return String_Of_tokens        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3f44b44",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vamos para praia comigo quando voce sair do trabalho, bjs e obrigado'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions('Vamos pra praia cmg qdo vc sair do trabalho, bjs e obg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157512b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Remove special characters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2d7b6fc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def removing_special_characters(tweet):\n",
    "    \"\"\"\n",
    "    Removing all the special characters except the one that is passed within \n",
    "    the regex to match, as they have important meaning in the text provided.\n",
    "    Also remove all html tags, hashtags but keepimg the word after (#covid)\n",
    "   \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text with removed special characters that don't require.\n",
    "        \n",
    "    Example: \n",
    "    Input : <IMG> K-a-j-a-l. #COVID #NBA Thi*s is $100.05 : @BRASIL @Barcelona recieve! (Is this okay?) \n",
    "    Output :  Hello, Kajal. This is $100.05 : the payment that you will recieve! Is this okay?\n",
    "    \n",
    "    \"\"\"\n",
    "    # The formatted text after removing not necessary punctuations.\n",
    "    Formatted_tweet = re.sub(r'@[A-Za-z0-9_]+','',tweet)  # remove @mentions  #[^a-zA-Z0-9:$-,%.?!]\n",
    "    Formatted_tweet = re.sub(r'#',' ',Formatted_tweet)  # remove hashtags   \n",
    "    Formatted_tweet = re.sub(r\"<[A-Za-z0-9_]+>\",'',tweet)  # remove tag's\n",
    "    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
    "    return Formatted_tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b2e32d0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' K-a-j-a-l. #COVID #NBA Thi*s is $100.05 : @BRASIL @Barcelona recieve! (Is this okay?)'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removing_special_characters('<IMG> K-a-j-a-l. #COVID #NBA Thi*s is $100.05 : @BRASIL @Barcelona recie<IMG>ve! (Is this okay?)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62c8c4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "baf372f8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# How to print Stopwords\n",
    "# from nltk.corpus import stopwords.\n",
    "# stops = set(stopwords.words('portuguese'))\n",
    "# print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ca6f49e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The code for removing stopwords\n",
    "def removing_stopwords(tweet):\n",
    "    \"\"\"\n",
    "    This function will remove stopwords which doesn't add much meaning to a sentence \n",
    "    & they can be remove safely without comprimising meaning of the sentence.\n",
    "    \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after omitted all stopwords.\n",
    "        \n",
    "    Example: \n",
    "    Input : hoje estou Barcelona estÃ¡ todos estamos voces estÃ£o agente ontem estive\n",
    "    Output : hoje Barcelona todos voces agente ontem \n",
    "    \n",
    "    \"\"\"\n",
    "    tweet = ' '.join([word for word in tweet.split() if word not in (stopwords.words('portuguese'))])\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ecbdefe",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hoje Barcelona todos voces agente ontem'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removing_stopwords('hoje estou Barcelona estÃ¡ todos estamos voces estÃ£o agente ontem estive')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5213480",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Dealing with stopwords... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc4740fb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'a',\n",
       " 'o',\n",
       " 'que',\n",
       " 'e',\n",
       " 'Ã©',\n",
       " 'do',\n",
       " 'da',\n",
       " 'em',\n",
       " 'um',\n",
       " 'para',\n",
       " 'com',\n",
       " 'nÃ£o',\n",
       " 'uma',\n",
       " 'os',\n",
       " 'no',\n",
       " 'se',\n",
       " 'na',\n",
       " 'por',\n",
       " 'mais',\n",
       " 'as',\n",
       " 'dos',\n",
       " 'como',\n",
       " 'mas',\n",
       " 'ao',\n",
       " 'ele',\n",
       " 'das',\n",
       " 'Ã ',\n",
       " 'seu',\n",
       " 'sua',\n",
       " 'ou',\n",
       " 'quando',\n",
       " 'muito',\n",
       " 'nos',\n",
       " 'jÃ¡',\n",
       " 'eu',\n",
       " 'tambÃ©m',\n",
       " 'sÃ³',\n",
       " 'pelo',\n",
       " 'pela',\n",
       " 'atÃ©',\n",
       " 'isso',\n",
       " 'ela',\n",
       " 'entre',\n",
       " 'depois',\n",
       " 'sem',\n",
       " 'mesmo',\n",
       " 'aos',\n",
       " 'seus',\n",
       " 'quem',\n",
       " 'nas',\n",
       " 'me',\n",
       " 'esse',\n",
       " 'eles',\n",
       " 'vocÃª',\n",
       " 'essa',\n",
       " 'num',\n",
       " 'nem',\n",
       " 'suas',\n",
       " 'meu',\n",
       " 'Ã s',\n",
       " 'minha',\n",
       " 'numa',\n",
       " 'pelos',\n",
       " 'elas',\n",
       " 'qual',\n",
       " 'nÃ³s',\n",
       " 'lhe',\n",
       " 'deles',\n",
       " 'essas',\n",
       " 'esses',\n",
       " 'pelas',\n",
       " 'este',\n",
       " 'dele',\n",
       " 'tu',\n",
       " 'te',\n",
       " 'vocÃªs',\n",
       " 'vos',\n",
       " 'lhes',\n",
       " 'meus',\n",
       " 'minhas',\n",
       " 'teu',\n",
       " 'tua',\n",
       " 'teus',\n",
       " 'tuas',\n",
       " 'nosso',\n",
       " 'nossa',\n",
       " 'nossos',\n",
       " 'nossas',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'esta',\n",
       " 'estes',\n",
       " 'estas',\n",
       " 'aquele',\n",
       " 'aquela',\n",
       " 'aqueles',\n",
       " 'aquelas',\n",
       " 'isto',\n",
       " 'aquilo',\n",
       " 'estou',\n",
       " 'estÃ¡',\n",
       " 'estamos',\n",
       " 'estÃ£o',\n",
       " 'estive',\n",
       " 'esteve',\n",
       " 'estivemos',\n",
       " 'estiveram',\n",
       " 'estava',\n",
       " 'estÃ¡vamos',\n",
       " 'estavam',\n",
       " 'estivera',\n",
       " 'estivÃ©ramos',\n",
       " 'esteja',\n",
       " 'estejamos',\n",
       " 'estejam',\n",
       " 'estivesse',\n",
       " 'estivÃ©ssemos',\n",
       " 'estivessem',\n",
       " 'estiver',\n",
       " 'estivermos',\n",
       " 'estiverem',\n",
       " 'hei',\n",
       " 'hÃ¡',\n",
       " 'havemos',\n",
       " 'hÃ£o',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houveram',\n",
       " 'houvera',\n",
       " 'houvÃ©ramos',\n",
       " 'haja',\n",
       " 'hajamos',\n",
       " 'hajam',\n",
       " 'houvesse',\n",
       " 'houvÃ©ssemos',\n",
       " 'houvessem',\n",
       " 'houver',\n",
       " 'houvermos',\n",
       " 'houverem',\n",
       " 'houverei',\n",
       " 'houverÃ¡',\n",
       " 'houveremos',\n",
       " 'houverÃ£o',\n",
       " 'houveria',\n",
       " 'houverÃ­amos',\n",
       " 'houveriam',\n",
       " 'sou',\n",
       " 'somos',\n",
       " 'sÃ£o',\n",
       " 'era',\n",
       " 'Ã©ramos',\n",
       " 'eram',\n",
       " 'fui',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'foram',\n",
       " 'fora',\n",
       " 'fÃ´ramos',\n",
       " 'seja',\n",
       " 'sejamos',\n",
       " 'sejam',\n",
       " 'fosse',\n",
       " 'fÃ´ssemos',\n",
       " 'fossem',\n",
       " 'for',\n",
       " 'formos',\n",
       " 'forem',\n",
       " 'serei',\n",
       " 'serÃ¡',\n",
       " 'seremos',\n",
       " 'serÃ£o',\n",
       " 'seria',\n",
       " 'serÃ­amos',\n",
       " 'seriam',\n",
       " 'tenho',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tÃ©m',\n",
       " 'tinha',\n",
       " 'tÃ­nhamos',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'teve',\n",
       " 'tivemos',\n",
       " 'tiveram',\n",
       " 'tivera',\n",
       " 'tivÃ©ramos',\n",
       " 'tenha',\n",
       " 'tenhamos',\n",
       " 'tenham',\n",
       " 'tivesse',\n",
       " 'tivÃ©ssemos',\n",
       " 'tivessem',\n",
       " 'tiver',\n",
       " 'tivermos',\n",
       " 'tiverem',\n",
       " 'terei',\n",
       " 'terÃ¡',\n",
       " 'teremos',\n",
       " 'terÃ£o',\n",
       " 'teria',\n",
       " 'terÃ­amos',\n",
       " 'teriam']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see the stop word list present in the NLTK library, without adding our custom list\n",
    "# Print the list of available languages\n",
    "# print(stopwords.fileids())\n",
    "\n",
    "# add custom list to stopword list of nltk\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "# stopwords.extend(new_stopwords)\n",
    "\n",
    "# Add a list of words\n",
    "# english_stopwords.extend(['food', 'meal', 'eat'])\n",
    "\n",
    "# english_stopwords = stopwords.words('english')\n",
    "portuguese_stopwords = stopwords.words('portuguese')\n",
    "portuguese_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff4adfed",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove a single word\n",
    "portuguese_stopwords.remove('nÃ£o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54bda8c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if it works...\n",
    "'nÃ£o'in portuguese_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8fd68a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Add a single word\n",
    "portuguese_stopwords.append('tua')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c981c14",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if it works...\n",
    "'tua'in portuguese_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbbf96bd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create our custom stopword list to add\n",
    "# Custom StopWords portuguese\n",
    "\n",
    "our_stopwords=['a','ah','g','h', 'd','ca','te','tu','tua','tuas','um','uma','voce','voces','vos', 'la','lo','lÃ¡',\n",
    "               'as','ao','aos','aquela','aquelas','aquele','aqueles','aquilo','as','ate','com','como','da','das',\n",
    "               'de','dela','delas','dele','deles','depois','do','dos','e','ela','elas','ele','eles','em','entre',\n",
    "               'essa','essas','esse','esses','eu','for','isso','isto','jÃ¡','lhe','lhes','me','mesmo','meu','meus',\n",
    "               'minha','minhas','muito','na','nas','no','nos','nossa','nossas','nosso','nossos','num','numa',\n",
    "               'nÃ³s','oh','o','os','para','pela','pelas','pelo','pelos','por','qual','quando','que','quem',\n",
    "               'se','sem','seu','seus','somos','sou','sua','suas','so','tambem', 'mas','ou', 'nem',\n",
    "               'este','teu','teus','estes','estas','agora','ai','alem','algo','alguÃ©m','algum','ainda',\n",
    "               'alguma','algumas','alguns', 'ali','ampla','amplas', 'amplo', 'amplos','ante', 'antes','apenas',\n",
    "               'apoio','apÃ³s','aqui','aquilo','assim','atrÃ¡s','atravÃ©s','bastante','breve','cada', 'cedo', 'cento',\n",
    "               'certamente','certeza','cima','coisa','coisas','da','dao','daquela', 'daquelas','daquele',\n",
    "               'daqueles','dentro','contudo','debaixo','demais','depois','desde','dessa','dessas','desse','desses',\n",
    "               'desta','destas','deste','destes','embora','enquanto','entre','etc','feita','feitas','feito',\n",
    "               'feitos','for','fora','geral','grande','grandes','hoje', 'hora', 'horas', 'longe',\n",
    "               'lugar', 'maior','maioria','mais','meio', 'menor', 'menos', 'mes', 'meses','mesma', 'mesmas',\n",
    "               'mesmo', 'mesmos','muita', 'muitas','muito','muitos','naquela', 'naquelas', 'naquele', 'naqueles',\n",
    "               'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes','num', 'numa','onde',\n",
    "               'ontem','perto','parte','outra', 'outras', 'outro', 'outros', 'pois', 'porÃ©m', 'porque',\n",
    "               'possivel', 'possivelmente','pouca', 'poucas', 'pouco', 'poucos', 'primeira', 'primeiras',\n",
    "               'primeiro', 'primeiros','propria','proprias','proprio', 'proprios', 'proxima', 'proximas',\n",
    "               'proximo', 'proximos','quais', 'quanto', 'quantos','quem','sempre','si', 'sido','sob', 'sobre',\n",
    "               'tal', 'talvez','tampouco', 'tanta', 'tantas','tanto', 'tao', 'tarde', 'te', 'todo', 'todos',\n",
    "               'toda', 'todas','tudo', 'ultima', 'ultimas', 'ultimo', 'ultimos','vÃ¡rios','vez', 'vezes',]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39588965",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Correct mis-spelled words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81da581a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# FUNCIONA MUY BIEN EN INGLES ... COMBINANDO CON EL nltk.stem.WordNetLemmatizer()\n",
    "# The code for spelling corrections \n",
    "def spelling_correction(tweet):\n",
    "    ''' \n",
    "    This function will correct spellings.\n",
    "    \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after corrected spellings.\n",
    "        \n",
    "    Example: \n",
    "    Input : voc e eu naum gostanos de brencar na rua sin salda\n",
    "    Output : vocÃª e eu num gostamos de brincar na rua se saia\n",
    "    \n",
    "    '''\n",
    "    # Check for spellings in Portuguese language\n",
    "    spell = Speller(lang='pt')  # English = 'en'\n",
    "    Corrected_tweet = spell(tweet)\n",
    "    return Corrected_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fbc4afa",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vocÃª e eu num gostamos de brincar na rua se saia'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spelling_correction('voc e eu naum gostanos de brencar na rrua sm saia')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2f9bf",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48940e8e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# La lematizaciÃ³n analiza el texto circundante para \n",
    "# determinar la parte del discurso de una palabra dada, no clasifica las frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4d131fc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 04:07:19 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "| pos       | bosque  |\n",
      "| lemma     | bosque  |\n",
      "| depparse  | bosque  |\n",
      "=======================\n",
      "\n",
      "2022-03-24 04:07:19 INFO: Use device: cpu\n",
      "2022-03-24 04:07:19 INFO: Loading: tokenize\n",
      "2022-03-24 04:07:19 INFO: Loading: mwt\n",
      "2022-03-24 04:07:19 INFO: Loading: pos\n",
      "2022-03-24 04:07:19 INFO: Loading: lemma\n",
      "2022-03-24 04:07:19 INFO: Loading: depparse\n",
      "2022-03-24 04:07:20 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('pt')  # set the language for Portuguese (stanza)\n",
    "# Stanza it is built with highly accurate neural network components that enable efficient \n",
    "# training and evaluation with your own annotated data,\n",
    "\n",
    "# The code for lemmatization\n",
    "def lemmatization(tweet):\n",
    "    \"\"\"\n",
    "    This function converts word to their root words \n",
    "    without explicitely cut down as done in stemming.\n",
    "   \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text having root words only, no tense form, no plural forms\n",
    "        \n",
    "    Example: \n",
    "    Input : brincando treino cantei jogarei subindo agredido\n",
    "    Output : 'brincar treinar cantar jogar subir agredir '\n",
    "    \n",
    "    \"\"\"\n",
    "    lemma = \"\"\n",
    "    for sent in nlp(tweet).sentences:\n",
    "        for word in sent.words:\n",
    "            lemma += word.lemma + \" \"\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33bd7eba",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brincar treinar cantar jogar subir agredir '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization('brincando treinou cantei jogarei subindo agredido')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21723b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "947174f0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# DESACTIVATE - We are using just Lemmatization for the moment...\n",
    "# stemmezation = False\n",
    "# The code for stemmezation\n",
    "stemmer = RSLPStemmer()  # A stemmer for Portuguese\n",
    "def stemmezation(tweet):\n",
    "    \"\"\"\n",
    "    This function essentially chops \n",
    "    off letters from the end until the stem is reached\n",
    "    it helps if the search returns variations of the word\n",
    "       \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text having root words only, no tense form, no plural forms\n",
    "        \n",
    "    Example: \n",
    "    Input : amor amante amando amar amado amei amore amamos amarei amo \n",
    "    Output : am  am     am     am   am    ame  amor  am     am     amo\n",
    "    \n",
    "   \"\"\"\n",
    "    # Converting words to their root forms\n",
    "    for token in tweet.split():\n",
    "        print(stemmer.stem(token))\n",
    "    return stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b87c2288",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "am\n",
      "am\n",
      "am\n",
      "am\n",
      "ame\n",
      "amor\n",
      "am\n",
      "am\n",
      "amo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nltk.stem.rslp.RSLPStemmer at 0x14e91b1f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmezation('amor amante amando amar amado amei amore amamos amarei amo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae55ad3c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def demo():\n",
    "    from nltk import stem\n",
    "    stemmer = stem.RSLPStemmer() \n",
    "\n",
    "    # white-space tokenizer friendly text\n",
    "\n",
    "    tweet_token = tweet.split()\n",
    "  \n",
    "    for token in tweet_token:\n",
    "        word = token\n",
    "        print(stemmer.stem(token))\n",
    "    \n",
    "        return stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5fd1a0ac",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nltk.stem.rslp.RSLPStemmer at 0x14e91b0d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45017ae7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Putting all in single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e87c0b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Writing main function to merge all the preprocessing steps.\n",
    "\n",
    "def text_preprocessing(tweet, lowercase=True, links=True, remove_html=False, \n",
    "                       numbers=True, special_chars=True, repeatition=True, \n",
    "                       newlines_tabs=True, punctuation=True, extra_whitespace=True, \n",
    "                       contractions=True, mis_spell=True, stop_words=True, \n",
    "                       lemmatization_word=True, stemmezation = False, \n",
    "                       accented_chars=True):\n",
    "    \"\"\"\n",
    "    This function will preprocess input text and return\n",
    "    the clean text.\n",
    "    \"\"\"\n",
    "        \n",
    "    if lowercase == True:  # convert all characters to lowercase.\n",
    "         tweet = lower_casing_text(tweet)\n",
    "            \n",
    "    if links == True:  # remove links.\n",
    "        tweet = remove_links(tweet) \n",
    "        \n",
    "    if remove_html == True: # remove html tags   # **DESACTIVATE\n",
    "        Data = strip_html_tags(Data)\n",
    "\n",
    "    if numbers == True:  # remove all numbers.\n",
    "        tweet = remove_number(tweet)\n",
    "    \n",
    "    if special_chars == True:  # remove all special characters.\n",
    "        tweet = removing_special_characters(tweet)\n",
    "        \n",
    "    if repeatition == True:  # reduce repeatitions.   \n",
    "        tweet = reducing_error_char_repeatation(tweet)\n",
    "        \n",
    "    if newlines_tabs == True:  # remove newlines & tabs.\n",
    "        tweet = remove_newlines_tabs(tweet)\n",
    "        \n",
    "    if punctuation == True:  # remove punctuation.   \n",
    "        tweet = remove_punctuation(tweet)\n",
    "        \n",
    "    if extra_whitespace == True:  # remove extra whitespaces.\n",
    "        tweet = remove_whitespaces(tweet)\n",
    "        \n",
    "    if contractions == True: # expand contractions.\n",
    "        tweet = expand_contractions(tweet)\n",
    "        \n",
    "    spell = Speller(lang='pt')\n",
    "    \n",
    "    if mis_spell == True: # check for mis-spelled words & correct them.\n",
    "        tweet = spelling_correction(tweet)\n",
    "        \n",
    "    stoplist = stopwords.words('portuguese') \n",
    "    stoplist = set(stoplist)\n",
    "    \n",
    "    if stop_words == True:  # remove stopwords.\n",
    "        tweet = removing_stopwords(tweet)\n",
    "        \n",
    "    if lemmatization_word == True:  # converts words to lemma form.\n",
    "        tweet = lemmatization(tweet)  \n",
    "                \n",
    "    if accented_chars == True:  # remove accented characters.\n",
    "        tweet = accented_characters_removal(tweet)\n",
    "        \n",
    "    word_tokens = word_tokenize(tweet) # tokenize tweet.\n",
    "\n",
    "#     stemmer = RSLPStemmer()      # **DESACTIVATE\n",
    "#   if stemmezation_word == True:  # converts words to stemmer form.  \n",
    "#         df = stemmezation(df)\n",
    "    \n",
    "           \n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd3a64d3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q lindo &lt;3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma instituiÃ§Ã£o que cuida de crianÃ§as que sofrem bullying, abuso e violÃªncia. pegaram a historia dele e fizeram um projeto pra ajudar crianÃ§as :) https://t.co/M2436o8y6E'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d43c54be",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lir',\n",
       " 'ela',\n",
       " 'fazer',\n",
       " 'zune',\n",
       " 'art',\n",
       " 'artista',\n",
       " 'desenhar',\n",
       " 'kirishima',\n",
       " 'todo',\n",
       " 'dinheiro',\n",
       " 'arrecadar',\n",
       " 'ir',\n",
       " 'instituicao',\n",
       " 'cuidar',\n",
       " 'crianca',\n",
       " 'sofrer',\n",
       " 'bullying',\n",
       " 'abuso',\n",
       " 'violencia',\n",
       " 'pegar',\n",
       " 'historia',\n",
       " 'fazer',\n",
       " 'projeto',\n",
       " 'ajudar',\n",
       " 'crianca']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessing(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c20623",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## To be continue .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37011b8b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing for Content\n",
    "\n",
    "# List_Content = df['tweet_text'].to_list()\n",
    "\n",
    "# Final_Article = []\n",
    "# Complete_Content = []\n",
    "\n",
    "# for article in List_Content:\n",
    "#     Processed_Content = text_preprocessing(article) # Cleaned text of Content attribute after pre-processing\n",
    "#     Final_Article.append(Processed_Content)\n",
    "\n",
    "# Complete_Content.extend(Final_Article)\n",
    "# df['Processed_Content'] = Complete_Content\n",
    "\n",
    "# # Pre-processing for Title\n",
    "\n",
    "# List_Title = df['Title'].to_list()\n",
    "\n",
    "# Final_Title = []\n",
    "# Complete_Title = []\n",
    "\n",
    "# for title in List_Title:\n",
    "#     Processed_Title = text_preprocessing(title) # Cleaned text of Title attribute after pre-processing\n",
    "#     Final_Title.append(Processed_Title)\n",
    "\n",
    "# Complete_Title.extend(Final_Title)\n",
    "# df['Processed_Title'] = Complete_Title \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "002128c0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379410e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f6fb2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
