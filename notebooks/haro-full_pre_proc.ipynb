{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002f9cc0",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464670da",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767ccf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8988ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/haroldinho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /Users/haroldinho/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import unidecode  # 1.7\n",
    "import pandas as pd  # for life!\n",
    "import re  # 1.5\n",
    "import string  # Common string operations\n",
    "import nltk  # 1.14\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize  # 1.12\n",
    "from nltk.corpus import stopwords  # 1.12\n",
    "from nltk.stem import WordNetLemmatizer # 1.14\n",
    "from nltk.stem import RSLPStemmer # 1.14\n",
    "nltk.download('rslp')\n",
    "from autocorrect import Speller # 1.13\n",
    "from bs4 import BeautifulSoup  # 1.4\n",
    "# from nltk import word_tokenize\n",
    "# import timeit\n",
    "# import time\n",
    "#from nltk.stem.api import StemmerI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f5f5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Data points:  159849\n",
      "Number of Features:  3\n",
      "features:  ['Unnamed: 0' 'tweet_text' 'encoded_sentiment']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@dilsonramoslima #Fato Acho que o Roger √© um b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#NOVIDADE! @LATAM_BRA acaba de anunciar novo v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Quando tem #novidadeüòÜ tem @novafm103 na √°rea! ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@RiodeNojeira #Novidade Ta√≠ o sucesso dos filh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Livro/Novidades] Segredos, uma hist√≥ria de Lu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         tweet_text  \\\n",
       "0           0  @dilsonramoslima #Fato Acho que o Roger √© um b...   \n",
       "1           1  #NOVIDADE! @LATAM_BRA acaba de anunciar novo v...   \n",
       "2           2  Quando tem #novidadeüòÜ tem @novafm103 na √°rea! ...   \n",
       "3           3  @RiodeNojeira #Novidade Ta√≠ o sucesso dos filh...   \n",
       "4           4  [Livro/Novidades] Segredos, uma hist√≥ria de Lu...   \n",
       "\n",
       "   encoded_sentiment  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Dataset\n",
    "df = pd.read_csv('../thermofeeler/data/encoded_df.csv')\n",
    "print('Number of Data points: ', df.shape[0])\n",
    "print('Number of Features: ', df.shape[1])\n",
    "print('features: ', df.columns.values)\n",
    "\n",
    "# Show Dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2c969a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@dilsonramoslima #Fato Acho que o Roger √© um b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#NOVIDADE! @LATAM_BRA acaba de anunciar novo v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quando tem #novidadeüòÜ tem @novafm103 na √°rea! ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  encoded_sentiment\n",
       "0  @dilsonramoslima #Fato Acho que o Roger √© um b...                  0\n",
       "1  #NOVIDADE! @LATAM_BRA acaba de anunciar novo v...                  0\n",
       "2  Quando tem #novidadeüòÜ tem @novafm103 na √°rea! ...                  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unnamed column\n",
    "df = df.drop(columns='Unnamed: 0')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be74bbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159849 entries, 0 to 159848\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   tweet_text         159849 non-null  object\n",
      " 1   encoded_sentiment  159849 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# This command tells information about the attributes of Dataset.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f694be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159849.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.816499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       encoded_sentiment\n",
       "count      159849.000000\n",
       "mean            0.000000\n",
       "std             0.816499\n",
       "min            -1.000000\n",
       "25%            -1.000000\n",
       "50%             0.000000\n",
       "75%             1.000000\n",
       "max             1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows statistics for every numerical column in our dataset.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1a595",
   "metadata": {},
   "source": [
    "## Check type of Dataframe attribute that has to processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937d11e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type of attribute \"tweet_text\"\n",
    "type(df['tweet_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2e8858c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type of attribute \"encoded_sentiment\"\n",
    "type(df['encoded_sentiment'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71834486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q lindo &lt;3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma institui√ß√£o que cuida de crian√ßas que sofrem bullying, abuso e viol√™ncia. pegaram a historia dele e fizeram um projeto pra ajudar crian√ßas :) https://t.co/M2436o8y6E'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = df['tweet_text'][85442]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a837797",
   "metadata": {},
   "source": [
    "## Remove newlines & tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c24f6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 'This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d11ccfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines_tabs(tweet):\n",
    "    \"\"\"  \n",
    "    This function will remove all the occurrences of newlines, tabs, and combinations like: \\\\n, \\\\.\n",
    "    \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of newlines, tabs, \\\\n, \\\\ characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\n",
    "    Output : This is her first day at this place. Please, Be nice to her. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n",
    "    Formatted_tweet = tweet.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "    return Formatted_tweet\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7374e50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q lindo &lt;3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma institui√ß√£o que cuida de crian√ßas que sofrem bullying, abuso e viol√™ncia. pegaram a historia dele e fizeram um projeto pra ajudar crian√ßas :) https://t.co/M2436o8y6E\n",
      "this tweet has 279 words\n"
     ]
    }
   ],
   "source": [
    "print(remove_newlines_tabs(tweet))\n",
    "print(f'this tweet has {len(tweet)} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45533125",
   "metadata": {},
   "source": [
    "## Strip Html Tags \n",
    "'< >'   for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2740da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = 'This is a nice place to live. <IMG>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2093ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(tweet):\n",
    "    \"\"\" \n",
    "    This function will remove all the occurrences of html tags from the text.\n",
    "    \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of html tags.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is a nice place to live. <IMG>\n",
    "    Output : This is a nice place to live.  \n",
    "    \"\"\"\n",
    "    # Initiating BeautifulSoup object soup.\n",
    "    soup = BeautifulSoup(tweet, \"html.parser\")\n",
    "    # Get all the text other than html tags.\n",
    "    stripped_tweet = soup.get_text(separator=\" \")\n",
    "    return stripped_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48345ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q lindo <3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma institui√ß√£o que cuida de crian√ßas que sofrem bullying, abuso e viol√™ncia. pegaram a historia dele e fizeram um projeto pra ajudar crian√ßas :) https://t.co/M2436o8y6E'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_html_tags(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a89b8",
   "metadata": {},
   "source": [
    "## Remove Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "343d6433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 'To know more about cats and food & website: catster.com  visit: https://catster.com//how-to-feed-cats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "814926c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    \"\"\"\n",
    "    This function will remove all the occurrences of links.\n",
    "    \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of all types of links.\n",
    "        \n",
    "    Example:\n",
    "    Input : To know more about cats and food & website: catster.com  visit: https://catster.com//how-to-feed-cats\n",
    "    Output : To know more about cats and food & website: visit:     \n",
    "    \n",
    "    \"\"\"\n",
    "    # Removing all the occurrences of links that starts with https\n",
    "    remove_https = re.sub(r'http\\S+', '', tweet)  # r'\\1' --> Limits all the repeatation to only one character.\n",
    "    # return remove_https                         \n",
    "    \n",
    "    # Remove all the occurrences of text that ends with .com\n",
    "    remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", ' ', remove_https)\n",
    "    return remove_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "085fbdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q lindo &lt;3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma institui√ß√£o que cuida de crian√ßas que sofrem bullying, abuso e viol√™ncia. pegaram a historia dele e fizeram um projeto pra ajudar crian√ßas :) '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_links(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59117494",
   "metadata": {},
   "source": [
    "## Remove WhiteSpaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98e5b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = 'How   are   you   doing   ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54181d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_white_spaces(tweet):\n",
    "    \"\"\" This function will remove \n",
    "        extra whitespaces from the text\n",
    "        \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after extra whitespaces removed .\n",
    "        \n",
    "    Example:\n",
    "    Input : How   are   you   doing   ?\n",
    "    Output : How are you doing ?     \n",
    "        \n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'\\s+')  # r'\\1' --> Limits all the repeatation to only one character.\n",
    "    Without_whitespace = re.sub(pattern, ' ', tweet)\n",
    "    # There are some instances where there is no space after '?' & ')', \n",
    "    # So I am replacing these with one space so that It will not consider two words as one token.\n",
    "    tweet = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85096788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q lindo &lt;3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma institui√ß√£o que cuida de crian√ßas que sofrem bullying, abuso e viol√™ncia. pegaram a historia dele e fizeram um projeto pra ajudar crian√ßas :)  https://t.co/M2436o8y6E'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_white_spaces(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ca49d",
   "metadata": {},
   "source": [
    "## Remove Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c782ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = 'M√°laga, √†√©√™√∂hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3696f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for accented characters removal\n",
    "def accented_characters_removal(tweet):\n",
    "    \"\"\"\n",
    "    The function will remove accented characters from the \n",
    "    text contained within the Dataset.\n",
    "       \n",
    "    arguments:\n",
    "        input_tweet: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" with removed accented characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : M√°laga, √†√©√™√∂hello\n",
    "    Output : Malaga, aeeohello    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove accented characters from text using unidecode.\n",
    "    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "    tweet = unidecode.unidecode(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b59ce91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q lindo &lt;3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma instituicao que cuida de criancas que sofrem bullying, abuso e violencia. pegaram a historia dele e fizeram um projeto pra ajudar criancas :) https://t.co/M2436o8y6E'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accented_characters_removal(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637c9cd",
   "metadata": {},
   "source": [
    "## Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6452e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for text lowercasing\n",
    "def lower_casing_text(tweet):\n",
    "    \"\"\"\n",
    "    The function will convert text into lower case.\n",
    "    \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "         value: text in lowercase\n",
    "         \n",
    "    Example:\n",
    "    Input : The World is Full of Surprises!\n",
    "    Output : the world is full of surprises!\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert text to lower case\n",
    "    # lower() - It converts all upperase letter of given string to lowercase.\n",
    "    tweet = tweet.lower()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b4ed883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q lindo &lt;3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma institui√ß√£o que cuida de crian√ßas que sofrem bullying, abuso e viol√™ncia. pegaram a historia dele e fizeram um projeto pra ajudar crian√ßas :) https://t.co/m2436o8y6e'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_casing_text(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2406c84",
   "metadata": {},
   "source": [
    "## Reduce repeated characters and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3799058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = 'Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a4a48fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for removing repeated characters and punctuations\n",
    "def reducing_incorrect_char_repeatation(tweet):\n",
    "    \"\"\"\n",
    "    This Function will reduce repeatition to two characters \n",
    "    for alphabets and to one character for punctuations.\n",
    "    \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Finally formatted text with alphabets repeating to \n",
    "        two characters & punctuations limited to one repeatition \n",
    "        \n",
    "    Example:\n",
    "    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
    "    Output : Reallyy, Greeaatt !?.;:)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Pattern matching for all case alphabets# Pattern matching for all case alphabets\n",
    "    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)  \n",
    "    # Limiting all the  repeatation to two characters.\n",
    "    Formatted_tweet = Pattern_alpha.sub(r\"\\1\\1\", tweet)   # r'\\1\\1' --> It limits all the repeatation to two characters.\n",
    "    # Pattern matching for all the punctuations that can occur\n",
    "    Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')  \n",
    "    \n",
    "    # Limiting punctuations in previously formatted string to only one.\n",
    "    Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_tweet)\n",
    "    \n",
    "    # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n",
    "    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)  # {2,} --> It means to match for repeatation that occurs more than two times\n",
    "    return Final_Formatted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ac7b693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q lindo &lt;3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma institui√ß√£o que cuida de crian√ßas que sofrem bullying, abuso e viol√™ncia. pegaram a historia dele e fizeram um projeto pra ajudar crian√ßas :) https:/t.co/M2436o8y6E'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducing_incorrect_char_repeatation(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca6ed7",
   "metadata": {},
   "source": [
    "## Expand contraction words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d85e12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laura ya tiene el mapeo de las palabras de contracci√≥n en portugues! ESTE ES UN TEST EN INGLES, FUNCIONA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e26824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = \"could've doesn't hadn't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eae59800",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "}\n",
    "\n",
    "# The code for expanding contraction words\n",
    "def expand_contractions(tweet, contraction_mapping =  CONTRACTION_MAP):\n",
    "    \"\"\"expand shortened words to the actual form.\n",
    "       e.g. don't to do not\n",
    "    \n",
    "       arguments:\n",
    "            input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "       return:\n",
    "            value: Text with expanded form of shorthened words.\n",
    "        \n",
    "       Example: \n",
    "       Input : ain't, aren't, can't, cause, can't've\n",
    "       Output :  is not, are not, cannot, because, cannot have \n",
    "    \n",
    "     \"\"\"\n",
    "    # Tokenizing text into tokens.\n",
    "    list_Of_tokens = tweet.split(' ')\n",
    "\n",
    "    # Checking for whether the given token matches with the Key & replacing word with key's value.\n",
    "    \n",
    "    # Check whether Word is in list_Of_tokens or not.\n",
    "    for Word in list_Of_tokens: \n",
    "        # Check whether found word is in dictionary \"Contraction Map\" or not as a key. \n",
    "         if Word in CONTRACTION_MAP: \n",
    "                # If Word is present in both dictionary & list_Of_tokens, replace that word with the key value.\n",
    "                list_Of_tokens = [item.replace(Word, CONTRACTION_MAP[Word]) for item in list_Of_tokens]\n",
    "                \n",
    "    # Converting list of tokens to String.\n",
    "    String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens) \n",
    "    return String_Of_tokens     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea8eaf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q lindo &lt;3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma institui√ß√£o que cuida de crian√ßas que sofrem bullying, abuso e viol√™ncia. pegaram a historia dele e fizeram um projeto pra ajudar crian√ßas :) https://t.co/M2436o8y6E'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(tweet, contraction_mapping =  CONTRACTION_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157512b",
   "metadata": {},
   "source": [
    "## Remove special characters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44aa9f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = 'Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2d7b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAY DE REPASAR\n",
    "def removing_special_characters(tweet):\n",
    "    \"\"\"Removing all the special characters except the one that is passed within \n",
    "       the regex to match, as they have important meaning in the text provided.\n",
    "   \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text with removed special characters that don't require.\n",
    "        \n",
    "    Example: \n",
    "    Input : Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \n",
    "    Output :  Hello, Kajal. This is $100.05 : the payment that you will recieve! Is this okay?\n",
    "    \n",
    "   \"\"\"\n",
    "    # The formatted text after removing not necessary punctuations.\n",
    "    Formatted_tweet = re.sub(r\"[^a-zA-Z$-√™]+\", ' ', tweet)   #[^a-zA-Z0-9:$-,%.?!]\n",
    "    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
    "    return Formatted_tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b2e32d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q lindo &lt;3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma institui√ß√£o que cuida de crian√ßas que sofrem bullying, abuso e viol√™ncia. pegaram a historia dele e fizeram um projeto pra ajudar crian√ßas :) https://t.co/M2436o8y6E'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removing_special_characters(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62c8c4",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43a313d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# He creado esta funcion para provar el uso del 'stopwords.words('portuguese')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b1186c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords.\n",
    "# stops = set(stopwords.words('english'))\n",
    "# print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ca6f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for removing stopwords\n",
    "stoplist = stopwords.words('portuguese') \n",
    "stoplist = set(stoplist)\n",
    "def removing_stopwords(tweet):\n",
    "    \"\"\"This function will remove stopwords which doesn't add much meaning to a sentence \n",
    "       & they can be remove safely without comprimising meaning of the sentence.\n",
    "    \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after omitted all stopwords.\n",
    "        \n",
    "    Example: \n",
    "    Input : This is Kajal from delhi who came here to study.\n",
    "    Output : [\"'This\", 'Kajal', 'delhi', 'came', 'study', '.', \"'\"] \n",
    "    \n",
    "   \"\"\"\n",
    "    # repr() function actually gives the precise information about the string\n",
    "    tweet = repr(tweet)\n",
    "    # Text without stopwords\n",
    "    No_StopWords = [word for word in word_tokenize(tweet) if word.lower() not in stoplist ]\n",
    "    # Convert list of tokens_without_stopwords to String type.\n",
    "    words_string = ' '.join(No_StopWords)    \n",
    "    return words_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7078d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"' q lindo & lt ; 3 fizeram zine/art 30 artistas desenhando kirishima todo dinheiro arrecadado vai pra institui√ß√£o cuida crian√ßas sofrem bullying , abuso viol√™ncia . pegaram historia fizeram projeto pra ajudar crian√ßas : ) https : //t.co/M2436o8y6E '\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removing_stopwords(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f52849ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'q lindo &lt;3 fizeram um zine/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma institui√ß√£o que cuida de crian√ßas que sofrem bullying, abuso e viol√™ncia. pegaram a historia dele e fizeram um projeto pra ajudar crian√ßas :) https://t.co/M2436o8y6E'\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98c136a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'a',\n",
       " 'o',\n",
       " 'que',\n",
       " 'e',\n",
       " '√©',\n",
       " 'do',\n",
       " 'da',\n",
       " 'em',\n",
       " 'um',\n",
       " 'para',\n",
       " 'com',\n",
       " 'n√£o',\n",
       " 'uma',\n",
       " 'os',\n",
       " 'no',\n",
       " 'se',\n",
       " 'na',\n",
       " 'por',\n",
       " 'mais',\n",
       " 'as',\n",
       " 'dos',\n",
       " 'como',\n",
       " 'mas',\n",
       " 'ao',\n",
       " 'ele',\n",
       " 'das',\n",
       " '√†',\n",
       " 'seu',\n",
       " 'sua',\n",
       " 'ou',\n",
       " 'quando',\n",
       " 'muito',\n",
       " 'nos',\n",
       " 'j√°',\n",
       " 'eu',\n",
       " 'tamb√©m',\n",
       " 's√≥',\n",
       " 'pelo',\n",
       " 'pela',\n",
       " 'at√©',\n",
       " 'isso',\n",
       " 'ela',\n",
       " 'entre',\n",
       " 'depois',\n",
       " 'sem',\n",
       " 'mesmo',\n",
       " 'aos',\n",
       " 'seus',\n",
       " 'quem',\n",
       " 'nas',\n",
       " 'me',\n",
       " 'esse',\n",
       " 'eles',\n",
       " 'voc√™',\n",
       " 'essa',\n",
       " 'num',\n",
       " 'nem',\n",
       " 'suas',\n",
       " 'meu',\n",
       " '√†s',\n",
       " 'minha',\n",
       " 'numa',\n",
       " 'pelos',\n",
       " 'elas',\n",
       " 'qual',\n",
       " 'n√≥s',\n",
       " 'lhe',\n",
       " 'deles',\n",
       " 'essas',\n",
       " 'esses',\n",
       " 'pelas',\n",
       " 'este',\n",
       " 'dele',\n",
       " 'tu',\n",
       " 'te',\n",
       " 'voc√™s',\n",
       " 'vos',\n",
       " 'lhes',\n",
       " 'meus',\n",
       " 'minhas',\n",
       " 'teu',\n",
       " 'tua',\n",
       " 'teus',\n",
       " 'tuas',\n",
       " 'nosso',\n",
       " 'nossa',\n",
       " 'nossos',\n",
       " 'nossas',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'esta',\n",
       " 'estes',\n",
       " 'estas',\n",
       " 'aquele',\n",
       " 'aquela',\n",
       " 'aqueles',\n",
       " 'aquelas',\n",
       " 'isto',\n",
       " 'aquilo',\n",
       " 'estou',\n",
       " 'est√°',\n",
       " 'estamos',\n",
       " 'est√£o',\n",
       " 'estive',\n",
       " 'esteve',\n",
       " 'estivemos',\n",
       " 'estiveram',\n",
       " 'estava',\n",
       " 'est√°vamos',\n",
       " 'estavam',\n",
       " 'estivera',\n",
       " 'estiv√©ramos',\n",
       " 'esteja',\n",
       " 'estejamos',\n",
       " 'estejam',\n",
       " 'estivesse',\n",
       " 'estiv√©ssemos',\n",
       " 'estivessem',\n",
       " 'estiver',\n",
       " 'estivermos',\n",
       " 'estiverem',\n",
       " 'hei',\n",
       " 'h√°',\n",
       " 'havemos',\n",
       " 'h√£o',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houveram',\n",
       " 'houvera',\n",
       " 'houv√©ramos',\n",
       " 'haja',\n",
       " 'hajamos',\n",
       " 'hajam',\n",
       " 'houvesse',\n",
       " 'houv√©ssemos',\n",
       " 'houvessem',\n",
       " 'houver',\n",
       " 'houvermos',\n",
       " 'houverem',\n",
       " 'houverei',\n",
       " 'houver√°',\n",
       " 'houveremos',\n",
       " 'houver√£o',\n",
       " 'houveria',\n",
       " 'houver√≠amos',\n",
       " 'houveriam',\n",
       " 'sou',\n",
       " 'somos',\n",
       " 's√£o',\n",
       " 'era',\n",
       " '√©ramos',\n",
       " 'eram',\n",
       " 'fui',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'foram',\n",
       " 'fora',\n",
       " 'f√¥ramos',\n",
       " 'seja',\n",
       " 'sejamos',\n",
       " 'sejam',\n",
       " 'fosse',\n",
       " 'f√¥ssemos',\n",
       " 'fossem',\n",
       " 'for',\n",
       " 'formos',\n",
       " 'forem',\n",
       " 'serei',\n",
       " 'ser√°',\n",
       " 'seremos',\n",
       " 'ser√£o',\n",
       " 'seria',\n",
       " 'ser√≠amos',\n",
       " 'seriam',\n",
       " 'tenho',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 't√©m',\n",
       " 'tinha',\n",
       " 't√≠nhamos',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'teve',\n",
       " 'tivemos',\n",
       " 'tiveram',\n",
       " 'tivera',\n",
       " 'tiv√©ramos',\n",
       " 'tenha',\n",
       " 'tenhamos',\n",
       " 'tenham',\n",
       " 'tivesse',\n",
       " 'tiv√©ssemos',\n",
       " 'tivessem',\n",
       " 'tiver',\n",
       " 'tivermos',\n",
       " 'tiverem',\n",
       " 'terei',\n",
       " 'ter√°',\n",
       " 'teremos',\n",
       " 'ter√£o',\n",
       " 'teria',\n",
       " 'ter√≠amos',\n",
       " 'teriam']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see the stop word list present in the NLTK library, without adding our custom list\n",
    "portuguese_stopwords = stopwords.words('portuguese')\n",
    "portuguese_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1c496a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove a single word\n",
    "portuguese_stopwords.remove('n√£o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc616d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'a',\n",
       " 'o',\n",
       " 'que',\n",
       " 'e',\n",
       " '√©',\n",
       " 'do',\n",
       " 'da',\n",
       " 'em',\n",
       " 'um',\n",
       " 'para',\n",
       " 'com',\n",
       " 'uma',\n",
       " 'os',\n",
       " 'no',\n",
       " 'se',\n",
       " 'na',\n",
       " 'por',\n",
       " 'mais',\n",
       " 'as',\n",
       " 'dos',\n",
       " 'como',\n",
       " 'mas',\n",
       " 'ao',\n",
       " 'ele',\n",
       " 'das',\n",
       " '√†',\n",
       " 'seu',\n",
       " 'sua',\n",
       " 'ou',\n",
       " 'quando',\n",
       " 'muito',\n",
       " 'nos',\n",
       " 'j√°',\n",
       " 'eu',\n",
       " 'tamb√©m',\n",
       " 's√≥',\n",
       " 'pelo',\n",
       " 'pela',\n",
       " 'at√©',\n",
       " 'isso',\n",
       " 'ela',\n",
       " 'entre',\n",
       " 'depois',\n",
       " 'sem',\n",
       " 'mesmo',\n",
       " 'aos',\n",
       " 'seus',\n",
       " 'quem',\n",
       " 'nas',\n",
       " 'me',\n",
       " 'esse',\n",
       " 'eles',\n",
       " 'voc√™',\n",
       " 'essa',\n",
       " 'num',\n",
       " 'nem',\n",
       " 'suas',\n",
       " 'meu',\n",
       " '√†s',\n",
       " 'minha',\n",
       " 'numa',\n",
       " 'pelos',\n",
       " 'elas',\n",
       " 'qual',\n",
       " 'n√≥s',\n",
       " 'lhe',\n",
       " 'deles',\n",
       " 'essas',\n",
       " 'esses',\n",
       " 'pelas',\n",
       " 'este',\n",
       " 'dele',\n",
       " 'tu',\n",
       " 'te',\n",
       " 'voc√™s',\n",
       " 'vos',\n",
       " 'lhes',\n",
       " 'meus',\n",
       " 'minhas',\n",
       " 'teu',\n",
       " 'tua',\n",
       " 'teus',\n",
       " 'tuas',\n",
       " 'nosso',\n",
       " 'nossa',\n",
       " 'nossos',\n",
       " 'nossas',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'esta',\n",
       " 'estes',\n",
       " 'estas',\n",
       " 'aquele',\n",
       " 'aquela',\n",
       " 'aqueles',\n",
       " 'aquelas',\n",
       " 'isto',\n",
       " 'aquilo',\n",
       " 'estou',\n",
       " 'est√°',\n",
       " 'estamos',\n",
       " 'est√£o',\n",
       " 'estive',\n",
       " 'esteve',\n",
       " 'estivemos',\n",
       " 'estiveram',\n",
       " 'estava',\n",
       " 'est√°vamos',\n",
       " 'estavam',\n",
       " 'estivera',\n",
       " 'estiv√©ramos',\n",
       " 'esteja',\n",
       " 'estejamos',\n",
       " 'estejam',\n",
       " 'estivesse',\n",
       " 'estiv√©ssemos',\n",
       " 'estivessem',\n",
       " 'estiver',\n",
       " 'estivermos',\n",
       " 'estiverem',\n",
       " 'hei',\n",
       " 'h√°',\n",
       " 'havemos',\n",
       " 'h√£o',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houveram',\n",
       " 'houvera',\n",
       " 'houv√©ramos',\n",
       " 'haja',\n",
       " 'hajamos',\n",
       " 'hajam',\n",
       " 'houvesse',\n",
       " 'houv√©ssemos',\n",
       " 'houvessem',\n",
       " 'houver',\n",
       " 'houvermos',\n",
       " 'houverem',\n",
       " 'houverei',\n",
       " 'houver√°',\n",
       " 'houveremos',\n",
       " 'houver√£o',\n",
       " 'houveria',\n",
       " 'houver√≠amos',\n",
       " 'houveriam',\n",
       " 'sou',\n",
       " 'somos',\n",
       " 's√£o',\n",
       " 'era',\n",
       " '√©ramos',\n",
       " 'eram',\n",
       " 'fui',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'foram',\n",
       " 'fora',\n",
       " 'f√¥ramos',\n",
       " 'seja',\n",
       " 'sejamos',\n",
       " 'sejam',\n",
       " 'fosse',\n",
       " 'f√¥ssemos',\n",
       " 'fossem',\n",
       " 'for',\n",
       " 'formos',\n",
       " 'forem',\n",
       " 'serei',\n",
       " 'ser√°',\n",
       " 'seremos',\n",
       " 'ser√£o',\n",
       " 'seria',\n",
       " 'ser√≠amos',\n",
       " 'seriam',\n",
       " 'tenho',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 't√©m',\n",
       " 'tinha',\n",
       " 't√≠nhamos',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'teve',\n",
       " 'tivemos',\n",
       " 'tiveram',\n",
       " 'tivera',\n",
       " 'tiv√©ramos',\n",
       " 'tenha',\n",
       " 'tenhamos',\n",
       " 'tenham',\n",
       " 'tivesse',\n",
       " 'tiv√©ssemos',\n",
       " 'tivessem',\n",
       " 'tiver',\n",
       " 'tivermos',\n",
       " 'tiverem',\n",
       " 'terei',\n",
       " 'ter√°',\n",
       " 'teremos',\n",
       " 'ter√£o',\n",
       " 'teria',\n",
       " 'ter√≠amos',\n",
       " 'teriam']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portuguese_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e5c18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # It returns a regular Python list\n",
    "# english_stopwords = stopwords.words('english')\n",
    "\n",
    "# # Add a single word\n",
    "# english_stopwords.append('plate')\n",
    "# Print the list of available languages\n",
    "# print(stopwords.fileids())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6970551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create our custom stopword list to add\n",
    "# new_stopwords = [\"all\", \"due\", \"to\", \"on\", \"daily\"]\n",
    "\n",
    "# # add custom list to stopword list of nltk\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "# stopwords.extend(new_stopwords)\n",
    "\n",
    "# Add a list of words\n",
    "# english_stopwords.extend(['food', 'meal', 'eat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "044378b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom StopWords portuguese\n",
    "# # stopwords: remove articles, prepositions, conjunctions etc\n",
    "#     stopwords=['a','te','tu','tua','tuas','t√©m','um','uma','voc√™','voc√™s','vos','√†','√†s','ao','aos',\n",
    "#           'aquela','aquelas','aquele','aqueles','aquilo','as','at√©','com','como','da','das','de',\n",
    "#           'dela','delas','dele','deles','depois','do','dos','e','ela','elas','ele','eles','em',\n",
    "#           'entre','essa','essas','esse','esses','esta','eu','foi','fomos','for','fora','foram',\n",
    "#           'forem','formos','fosse','fossem','fui','f√¥ramos','f√¥ssemos', 'isso','isto','j√°','lhe',\n",
    "#           'lhes','me','mesmo','meu','meus','minha','minhas','muito','na','nas','no','nos','nossa',\n",
    "#           'nossas','nosso','nossos','num','numa','n√≥s','o','os','para','pela','pelas','pelo','pelos',\n",
    "#           'por','qual','quando','que','quem','se','seja','sejam','sejamos','sem','serei','seremos',\n",
    "#           'seria','seriam','ser√°','ser√£o','ser√≠amos','seu','seus','somos','sou','sua','suas','s√£o',\n",
    "#           's√≥','tamb√©m']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39588965",
   "metadata": {},
   "source": [
    "## Correct mis-spelled words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81da581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONA MUY BIEN EN INGLES ... COMBINANDO CON EL nltk.stem.WordNetLemmatizer()\n",
    "# The code for spelling corrections \n",
    "def spelling_correction(tweet):\n",
    "    ''' \n",
    "    This function will correct spellings.\n",
    "    \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after corrected spellings.\n",
    "        \n",
    "    Example: \n",
    "    Input : This is Oberois from Dlhi who came heree to studdy.\n",
    "    Output : This is Oberoi from Delhi who came here to study.\n",
    "      \n",
    "    \n",
    "    '''\n",
    "    # Check for spellings in English language\n",
    "    spell = Speller(lang='pt')\n",
    "    Corrected_tweet = spell(tweet)\n",
    "    return Corrected_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4fbc4afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q lindo &la;3 fizeram um zune/art com 30 artistas desenhando o kirishima e todo o dinheiro arrecadado vai pra uma institui√ß√£o que cuida de crian√ßas que sofrem bullying, abuso e viol√™ncia. pegaram a historia dele e fizeram um projeto pra ajudar crian√ßas :) https://t.co/M2436o8y6E'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spelling_correction(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2f9bf",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3adbeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La lematizaci√≥n analiza el texto circundante para \n",
    "# determinar la parte del discurso de una palabra dada, no clasifica las frases.\n",
    "# no funciona tan bien para nuestro caso!! solamente si optamos anadir las buscas en ingles :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a626f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_en = 'textting reduced ... functionality updated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f109c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for lemmatization\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer() # Tokenize a string on whitespace (space, tab, newline)\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()  # Returns the input word unchanged if it cannot be found in WordNet.\n",
    "def lemmatization(tweet_en):\n",
    "    \"\"\"This function converts word to their root words \n",
    "       without explicitely cut down as done in stemming.\n",
    "    \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text having root words only, no tense form, no plural forms\n",
    "        \n",
    "    Example: \n",
    "    Input : text reduced \n",
    "    Output :  text reduce\n",
    "    \n",
    "   \"\"\"\n",
    "    # Converting words to their root forms\n",
    "    lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(tweet_en)]  # 'v' = verb\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33bd7eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['textting', 'reduce', '...', 'functionality', 'update']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization(tweet_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f3a93",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37aff9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"amor amante amando amar amado amei amore amamos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "947174f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for lemmatization\n",
    "stemmer = RSLPStemmer()  # A stemmer for Portuguese\n",
    "def stemmezation(tweet):\n",
    "    \"\"\"This function essentially chops \n",
    "       off letters from the end until the stem is reached\n",
    "       it helps if the search returns variations of the word\n",
    "       \n",
    "    arguments:\n",
    "         input_tweet: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text having root words only, no tense form, no plural forms\n",
    "        \n",
    "    Example: \n",
    "    Input : text reduced \n",
    "    Output :  text reduce\n",
    "    \n",
    "   \"\"\"\n",
    "    # Converting words to their root forms\n",
    "    for token in tweet.split():\n",
    "        print(stemmer.stem(token))\n",
    "    return stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b87c2288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "am\n",
      "am\n",
      "am\n",
      "am\n",
      "ame\n",
      "amor\n",
      "am\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nltk.stem.rslp.RSLPStemmer at 0x147ee1280>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmezation(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45017ae7",
   "metadata": {},
   "source": [
    "## Putting all in single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e87c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing main function to merge all the preprocessing steps.\n",
    "def text_preprocessing(tweet, accented_chars=True, contractions=True, lemmatization = True,\n",
    "                        extra_whitespace=True, newlines_tabs=True, repeatition=True, \n",
    "                       lowercase=True, punctuations=True, mis_spell=True,\n",
    "                       remove_html=True, links=True,  special_chars=True,\n",
    "                       stop_words=True):\n",
    "    \"\"\"\n",
    "    This function will preprocess input text and return\n",
    "    the clean text.\n",
    "    \"\"\"\n",
    "        \n",
    "    if newlines_tabs == True: # remove newlines & tabs.\n",
    "        Data = remove_newlines_tabs(text)\n",
    "        \n",
    "    if remove_html == True: #remove html tags\n",
    "        Data = strip_html_tags(Data)\n",
    "        \n",
    "    if links == True: #remove links\n",
    "        Data = remove_links(Data)\n",
    "        \n",
    "    if extra_whitespace == True: #remove extra whitespaces\n",
    "        Data = remove_whitespace(Data)\n",
    "        \n",
    "    if accented_chars == True: #remove accented characters\n",
    "        Data = accented_characters_removal(Data)\n",
    "        \n",
    "    if lowercase == True: #convert all characters to lowercase\n",
    "        Data = lower_casing_text(Data)\n",
    "        \n",
    "    if repeatition == True: #Reduce repeatitions   \n",
    "        Data = reducing_incorrect_character_repeatation(Data)\n",
    "        \n",
    "    if contractions == True: #expand contractions\n",
    "        Data = expand_contractions(Data)\n",
    "    \n",
    "    if punctuations == True: #remove punctuations\n",
    "        Data = removing_special_characters(Data)\n",
    "    \n",
    "    stoplist = stopwords.words('english') \n",
    "    stoplist = set(stoplist)\n",
    "    \n",
    "    if stop_words == True: #Remove stopwords\n",
    "        Data = removing_stopwords(Data)\n",
    "        \n",
    "    spell = Speller(lang='en')\n",
    "    \n",
    "    if mis_spell == True: #Check for mis-spelled words & correct them.\n",
    "        Data = spelling_correction(Data)\n",
    "        \n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "     \n",
    "    if lemmatization == True: #Converts words to lemma form.\n",
    "        Data = lemmatization(Data)\n",
    "    \n",
    "           \n",
    "    return Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
